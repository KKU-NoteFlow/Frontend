from __future__ import annotations
import os, json, re, asyncio
from typing import List, Literal, Optional, Dict, Any

# HF Transformers
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# schemasì—ì„œ ì •ì˜í•  ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from schemas import QuestionItem

# =========================================================
# ì„¤ì •ê°’ (Qwen3-4B-Instruct-2507 ëª¨ë¸ ì‚¬ìš©)
# =========================================================
QG_MODEL_NAME = os.getenv("QG_MODEL_NAME", "Qwen/Qwen3-4B-Instruct-2507") 
QG_MAX_NEW_TOKENS = int(os.getenv("QG_MAX_NEW_TOKENS", "4096"))
QG_TEMPERATURE = float(os.getenv("QG_TEMPERATURE", "0.2"))
HF_API_TOKEN = os.getenv("HF_API_TOKEN") or None

_QG_MODEL = None
_QG_TOKENIZER = None

# =========================================================
# ëª¨ë¸ ë¡œë”© ìœ í‹¸ (GPU 0 ë° CPU ë¶„ì‚° ë°°ì¹˜ ì ìš©)
# =========================================================
def _resolve_dtype():
    # bf16 ì§€ì› ì‹œ bf16, ì•„ë‹ˆë©´ fp16 ì‚¬ìš©
    if torch.cuda.is_available():
        if torch.cuda.is_bf16_supported():
            return torch.bfloat16
        return torch.float16
    return None

def _load_qg_model():
    """Qwen3-4B ëª¨ë¸ì„ GPU 0ê³¼ CPUì— ë¶„ì‚° ë¡œë“œí•©ë‹ˆë‹¤."""
    global _QG_MODEL, _QG_TOKENIZER
    if _QG_MODEL is not None and _QG_TOKENIZER is not None:
        return _QG_MODEL, _QG_TOKENIZER

    torch_dtype = _resolve_dtype()
    load_in_4bit = os.getenv("HF_LOAD_IN_4BIT", "false").lower() in ("1", "true", "yes")

    print(f"QG ëª¨ë¸ ë¡œë”© ì¤‘: {QG_MODEL_NAME}, 4bit={load_in_4bit}")

    hub_kwargs = {"trust_remote_code": True}
    if HF_API_TOKEN:
        hub_kwargs["token"] = HF_API_TOKEN
        
    try:
        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        _QG_TOKENIZER = AutoTokenizer.from_pretrained(QG_MODEL_NAME, **hub_kwargs)

        # 2. ëª¨ë¸ ë¡œë“œ ì„¤ì • (kwargs ì¤€ë¹„)
        kwargs = dict(hub_kwargs)
        if load_in_4bit:
            try:
                kwargs.update({"load_in_4bit": True})
            except Exception:
                pass
        
        if not load_in_4bit and torch_dtype is not None:
            kwargs["dtype"] = torch_dtype
                
        # ìƒì„±ì ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ dtype ì¸ìë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì œê±°
        kwargs.pop("dtype", None) 

        # ğŸ›‘ GPU 0ê³¼ CPUì—ë§Œ ëª…ì‹œì ìœ¼ë¡œ ë©”ëª¨ë¦¬ í• ë‹¹
        if torch.cuda.is_available():
            max_memory = {
                0: '23GiB',     
                'cpu': '30GiB', 
            }
            device_map = 'auto' 
            kwargs['max_memory'] = max_memory
            print(f"GPU 0 ë° CPU ë¶„ì‚° ë°°ì¹˜ ì ìš©: {max_memory}")
        else:
            device_map = 'auto'

        # 3. ëª¨ë¸ ë¡œë“œ (ëª…ì‹œì  ë¶„ì‚° ë°°ì¹˜ ì ìš©)
        _QG_MODEL = AutoModelForCausalLM.from_pretrained(
            QG_MODEL_NAME, 
            device_map=device_map,
            **kwargs
        )
        
        return _QG_MODEL, _QG_TOKENIZER
    except Exception as e:
        print(f"QG ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"QG ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {QG_MODEL_NAME} (ì˜¤ë¥˜ ë‚´ìš©: {e})") from e

# =========================================================
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ / í”„ë¡¬í”„íŠ¸ ë¹Œë“œ (ë³€ê²½ ì—†ìŒ)
# =========================================================
def _extract_key_points(full_text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ 'í•µì‹¬ ìš”ì ' ì„¹ì…˜ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    import re
    
    match = re.search(
        r"##\s*(í•µì‹¬ ìš”ì |KEY POINTS|KEY TAKEAWAYS)\s*\n(.*?)(\n##\s*|\Z)", 
        full_text, 
        re.DOTALL | re.IGNORECASE
    )
    
    if match:
        return "í•µì‹¬ ìš”ì :\n" + match.group(2).strip()
    
    print("ê²½ê³ : 'í•µì‹¬ ìš”ì ' ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œ ìƒì„± ì§€ì‹œ.")
    return full_text


def _build_qg_prompt(tokenizer, system_text: str, user_text: str) -> str:
    """í† í¬ë‚˜ì´ì €ì˜ ì±— í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    messages = [
        {"role": "system", "content": system_text},
        {"role": "user", "content": user_text},
    ]
    return tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )

def _get_system_prompt(num: int, q_type: str, lang: str) -> str:
    """ë¬¸ì œ ìƒì„±ì— íŠ¹í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    lang_name = "í•œêµ­ì–´" if lang == "ko" else "English"
    q_type_desc = "ê°ê´€ì‹ (4ì§€ì„ ë‹¤, ì •ë‹µ í¬í•¨)" if q_type == "multiple_choice" else "ë‹¨ë‹µí˜• ì£¼ê´€ì‹"
    
    return (
        f"ë‹¹ì‹ ì€ '{lang_name}'ë¡œ ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ êµìœ¡ìš© ë¬¸ì œë¥¼ ìƒì„±í•˜ëŠ” ë´‡ì…ë‹ˆë‹¤. "
        "ì…ë ¥ í…ìŠ¤íŠ¸ì˜ 'í•µì‹¬ ìš”ì ' ì„¹ì…˜ì— ëª…ì‹œëœ ì‚¬ì‹¤ë§Œì„ ê·¼ê±°ë¡œ ë¬¸ì œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. "
        f"{num}ê°œì˜ '{q_type_desc}' ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ **ë°˜ë“œì‹œ** JSON ë°°ì—´ í˜•ì‹ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ì‚¬ì¡±ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. "
        
        "\n\nJSON í˜•ì‹: "
        "[\n  {\n    \"question\": \"[ì§ˆë¬¸ ë‚´ìš©]\",\n    \"answer\": \"[ì •ë‹µ]\",\n    \"options\": [\"[ë³´ê¸° 1]\", \"[ë³´ê¸° 2]\", \"[ë³´ê¸° 3]\", \"[ë³´ê¸° 4]\"]  // ê°ê´€ì‹ì¼ ê²½ìš°\n  }\n]"
        "\n\nê·œì¹™:\n"
        "1. ì •ë‹µì€ ë°˜ë“œì‹œ 'options' ë¦¬ìŠ¤íŠ¸ ë‚´ì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
        "2. ì§ˆë¬¸ì€ ëª…í™•í•˜ê³ , ì •ë‹µì€ 'í•µì‹¬ ìš”ì 'ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "3. ë§ˆí¬ë‹¤ìš´(```` `)ì´ë‚˜ ë‹¤ë¥¸ ê¾¸ë°ˆ ì—†ì´ ìˆœìˆ˜ JSON ë°°ì—´ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤."
    )

# =========================================================
# ë©”ì¸ ë¬¸ì œ ìƒì„± í•¨ìˆ˜ (ë¬¸ì œ/ë‹µì•ˆ íŒŒì¼ ì €ì¥ ë¡œì§ í¬í•¨)
# =========================================================
async def generate_questions_from_text(
    text: str,
    num_questions: int,
    question_type: Literal["multiple_choice", "short_answer"],
    language: Literal["ko", "en"],
) -> List[QuestionItem]:
    
    # 1. ëª¨ë¸ ë¡œë“œ
    try:
        model, tokenizer = _load_qg_model()
    except RuntimeError:
        raise
        
    # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (í•µì‹¬ ìš”ì  ì¶”ì¶œ)
    key_points_only = _extract_key_points(text)
    
    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = _get_system_prompt(num_questions, question_type, language)
    user_payload = f"ë‹¤ìŒ 'í•µì‹¬ ìš”ì ' í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤:\n\n---\n{key_points_only}"
    prompt = _build_qg_prompt(tokenizer, system_prompt, user_payload)

    # 4. LLM ì¶”ë¡  (ë¹„ë™ê¸° ì²˜ë¦¬)
    def _generate_sync():
        """ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ë™ê¸° í•¨ìˆ˜"""
        inputs = tokenizer(prompt, return_tensors="pt")
        
        target_device = getattr(model, "device", "cuda:0")
        if torch.cuda.is_available():
            try:
                inputs = {k: v.to(target_device) for k, v in inputs.items()}
            except Exception:
                pass

        gen_kwargs = dict(
            max_new_tokens=QG_MAX_NEW_TOKENS,
            do_sample=True,
            temperature=QG_TEMPERATURE,
            repetition_penalty=1.05,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id,
        )
        
        with torch.no_grad():
            out = model.generate(**inputs, **gen_kwargs)
            
        gen_ids = out[0, inputs["input_ids"].shape[1]:]
        return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

    # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
    loop = asyncio.get_event_loop()
    raw_json_output = await loop.run_in_executor(None, _generate_sync)
    
    # 5. JSON íŒŒì‹± ë° ë°ì´í„° ì •ë¦¬ + íŒŒì¼ ì €ì¥
    try:
        if raw_json_output.startswith("```"):
            raw_json_output = re.sub(r"^```json\s*", "", raw_json_output, flags=re.IGNORECASE)
            raw_json_output = re.sub(r"```\s*$", "", raw_json_output)
        
        json_data: List[Dict[str, Any]] = json.loads(raw_json_output)
        
        # Pydantic ëª¨ë¸ë¡œ ë³€í™˜í•˜ì—¬ ë¬¸ì œ/ë‹µì•ˆ ë°ì´í„°ë¥¼ ì¤€ë¹„
        question_items = [QuestionItem(**item) for item in json_data]
        
        # Prepare file-friendly 'note' objects (title + content) for questions and answers
        questions_for_file = []
        answers_for_file = []
        
        for idx, item in enumerate(question_items, 1):
            # Build markdown content for question note
            md_lines = []
            md_lines.append(f"### ë¬¸ì œ {idx}")
            md_lines.append("")
            md_lines.append(item.question)
            md_lines.append("")
            if item.options:
                # numbered options
                for opt_idx, opt in enumerate(item.options, 1):
                    md_lines.append(f"{opt_idx}. {opt}")
            question_md = "\n".join(md_lines)
            # ë¬¸ì œ ê²½ê³„ êµ¬ë¶„ì„  ì¶”ê°€
            question_md = question_md + "\n\n---\n\n"

            questions_for_file.append({
                "title": f"ë¬¸ì œ {idx}",
                "content": question_md
            })

            # Build markdown content for answer note
            ans_lines = []
            ans_lines.append(f"### ë‹µì•ˆ {idx}")
            ans_lines.append("")
            # Provide a short preview of the question followed by the correct answer
            preview = (item.question[:200] + '...') if len(item.question) > 200 else item.question
            ans_lines.append(f"ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°: {preview}")
            ans_lines.append("")
            ans_lines.append(f"ì •ë‹µ: {item.answer}")
            answer_content = "\n".join(ans_lines)
            # ë‹µì•ˆ ê²½ê³„ êµ¬ë¶„ì„  ì¶”ê°€
            answer_content = answer_content + "\n\n---\n\n"
            answers_for_file.append({
                "title": f"ë‹µì•ˆ {idx}",
                "content": answer_content
            })

        # --- qg_questions.json ì €ì¥ (note-ready í˜•ì‹) ---
        output_q_filename = "qg_questions.json"
        # --- qg_answers.json ì €ì¥ (note-ready í˜•ì‹) ---
        output_a_filename = "qg_answers.json"

        async def _write_json_file(path: str, obj: Any):
            loop = asyncio.get_event_loop()
            def _sync_write():
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(obj, f, ensure_ascii=False, indent=4)
            try:
                await loop.run_in_executor(None, _sync_write)
                return True
            except Exception as e:
                print(f"[ERROR] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({path}): {e}")
                return False

        # ë¹„ë™ê¸°(ìŠ¤ë ˆë“œ í’€)ë¡œ íŒŒì¼ ì“°ê¸°
        write_q_ok = await _write_json_file(output_q_filename, {"notes": questions_for_file})
        if write_q_ok:
            print(f"[INFO] ë¬¸ì œ íŒŒì¼ì´ '{output_q_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        write_a_ok = await _write_json_file(output_a_filename, {"notes": answers_for_file})
        if write_a_ok:
            print(f"[INFO] ë‹µì•ˆ íŒŒì¼ì´ '{output_a_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        # Pydantic ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (API ì‘ë‹µ í˜•ì‹ì€ QuestionItemì„ ìœ ì§€)
        return question_items

    except json.JSONDecodeError as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\nì›ë³¸ ì¶œë ¥:\n{raw_json_output}")
        raise ValueError(f"LLM ì¶œë ¥ JSON íŒŒì‹± ì‹¤íŒ¨: {raw_json_output[:100]}...")
    except Exception as e:
        print(f"ìµœì¢… ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise
